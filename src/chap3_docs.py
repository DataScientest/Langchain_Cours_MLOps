import os, random
from src.documents.loaders import load_pdf
from src.documents.cleaners import clean_text
from src.documents.splitters import split_documents
from src.documents.search import keyword_search
from src.utils.token import count_tokens

print("\n=== D√©mo compl√®te des fonctions de traitement documents ===\n")
# --- 1. S√©lection al√©atoire d‚Äôun PDF ---
pdf_dir = "data/pdf"
pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith(".pdf")]
if not pdf_files:
    raise FileNotFoundError("Aucun PDF trouv√© dans data/pdf !")

pdf_path = os.path.join(pdf_dir, random.choice(pdf_files))
print(f"üìÑ Fichier choisi : {pdf_path}")

# --- 2. Charger le PDF ---
docs = load_pdf(pdf_path)
print(f"‚úÖ PDF charg√© : {len(docs)} pages")

# --- 3. Nettoyer le contenu de chaque page ---
for d in docs:
    d.page_content = clean_text(d.page_content)
print("‚úÖ Nettoyage effectu√©")

# --- 4. Split en chunks de 600 tokens ---
chunks = split_documents(docs, max_tokens=600, overlap_sentences=2)
print(f"‚úÖ Split en {len(chunks)} chunks (~600 tokens chacun)")

# --- 5. Lancer une recherche simple ---
query = "John McCarthy"
results = keyword_search(chunks, query, k=3)

print(f"\nüîç R√©sultats pour la requ√™te: {query}\n")
for i, r in enumerate(results, 1):
    print(f"--- R√©sultat {i} ---\n{r}\n")

# --- 6. V√©rifier la taille des premiers chunks ---
print("\n=== Aper√ßu des premiers chunks ===")
for i, c in enumerate(chunks):
    print(f"\n--- Chunk {i+1} ({count_tokens(c.page_content)} tokens) ---")
    print(c.page_content)